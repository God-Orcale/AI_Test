{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNncAtJ3eRjkzA6rhrcW8Ul",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/God-Orcale/AI_Test/blob/main/NetWork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "We-6ByuOQzHz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "# 构建一个神经网络来对FashionMNIST数据集中的图像进行分类"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"using {device}\")\n",
        "# 如果当前加速器可用，我们将使用它。否则，我们使用 CPU。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJEY0EWORJHm",
        "outputId": "56dd7316-dc43-44b6-9f70-d7604853400d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,10),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "# 通过子类化来定义我们的神经网络，并且在其中初始化神经网络层"
      ],
      "metadata": {
        "id": "nPqWwjQ_RkyF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "# 创建一个NeuralNetwork的实例，并将其移动到device中，然后打印它的结构。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDinaTD-SY-O",
        "outputId": "da736de5-f0f8-4cd4-d1c3-e886e9440832"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= torch.rand(1,28,28,device = device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class:{y_pred}\")\n",
        "# 在输入上调用模型将返回一个二维张量，其中dim=0对应于每个类的10个原始预测值的每个输出，dim=1对应于每个输出的单个值。\n",
        "# 我们通过模块nn.Softmax的一个实例传递预测概率来获得预测概率。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfuZUwrKTBu-",
        "outputId": "53a4be66-e3e3-432c-9a61-3493e07fbb70"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class:tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFuxwxbHTdmP",
        "outputId": "530f3421-1ec8-4c09-96a1-80b5536f6c89"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())\n",
        "# 我们初始化nn.拼合图层，将每个2D 28x28图像转换为包含784个像素值的连续数组"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We0wX4tRUE_K",
        "outputId": "31638e5b-384e-40c7-ce1d-8bffc88748b1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = nn.Linear(in_features= 28*28,out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())\n",
        "# 线性层是一个模块，它使用其存储的权重和偏差对输入应用线性变换。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppalt53KUY_2",
        "outputId": "ce749d9f-4133-45f9-bcdd-daa87ec7053f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before ReLU{hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU{hidden1}\")\n",
        "# 非线性激活是在模型的输入和输出之间创建复杂的映射。\n",
        "# 它们在线性变换后应用，以引入非线性，从而帮助神经网络学习各种各样的现象。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZlmxbgAUqfz",
        "outputId": "b345b14f-9dd5-4e8b-e113-9f334674206d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLUtensor([[-7.3235e-02,  2.2883e-02, -2.6995e-01, -1.4059e-01, -4.7971e-01,\n",
            "          2.0971e-02, -6.9681e-03, -1.1611e-01,  2.9156e-01,  3.0626e-01,\n",
            "         -5.0002e-01,  4.5415e-01,  2.1606e-01, -4.6669e-04,  8.5264e-02,\n",
            "          2.7805e-01,  4.2091e-01,  4.8029e-01, -4.9900e-01, -7.0524e-01],\n",
            "        [-1.3606e-01, -1.5751e-01, -3.2267e-01, -1.7436e-01, -4.3722e-01,\n",
            "         -1.3772e-01,  3.0266e-01,  1.4383e-01, -3.7931e-02,  1.0903e-01,\n",
            "         -5.6980e-01,  4.4693e-01,  1.0970e-01,  1.4063e-01,  1.8348e-01,\n",
            "         -7.0629e-02,  2.2586e-01,  3.1600e-01, -1.7524e-01, -1.8432e-01],\n",
            "        [-2.4502e-01,  2.7281e-01, -1.1779e-01, -3.4298e-01, -4.2495e-02,\n",
            "         -2.7655e-02,  6.6275e-02,  1.4935e-02, -1.3335e-02,  4.6180e-01,\n",
            "         -4.2545e-01,  5.2105e-01, -1.1341e-01,  2.1428e-01,  3.5837e-02,\n",
            "          4.8735e-01,  2.1930e-01,  1.8346e-01,  1.0707e-02,  3.9566e-02]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLUtensor([[0.0000, 0.0229, 0.0000, 0.0000, 0.0000, 0.0210, 0.0000, 0.0000, 0.2916,\n",
            "         0.3063, 0.0000, 0.4542, 0.2161, 0.0000, 0.0853, 0.2781, 0.4209, 0.4803,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3027, 0.1438, 0.0000,\n",
            "         0.1090, 0.0000, 0.4469, 0.1097, 0.1406, 0.1835, 0.0000, 0.2259, 0.3160,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0000, 0.2728, 0.0000, 0.0000, 0.0000, 0.0000, 0.0663, 0.0149, 0.0000,\n",
            "         0.4618, 0.0000, 0.5210, 0.0000, 0.2143, 0.0358, 0.4873, 0.2193, 0.1835,\n",
            "         0.0107, 0.0396]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "# 神经网络的最后一个线性层返回logits（原始值），这些值将传递给nn.Softmax模块。\n",
        "# logit 将缩放为值 [0， 1] 表示模型对每个类的预测概率。"
      ],
      "metadata": {
        "id": "D5fEh4ksVOgA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model structure{model}\\n\\n\")\n",
        "for name,param in model.named_parameters():\n",
        "  print(f\"Layer:{name}|size:{param.size()}|Values:{param[:2]}\\n\")\n",
        "# 神经网络中的许多层都是参数化的，即具有相关的权重以及在训练期间优化的偏差。\n",
        "# 自动子类化跟踪模型对象中定义的所有字段，并生成所有参数可使用您的模型或方法访问\n",
        "# nn.Module parameters() named_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYkY-5WuVtTO",
        "outputId": "856397f4-0c42-49cb-975e-91185cb8b0ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structureNeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer:linear_relu_stack.0.weight|size:torch.Size([512, 784])|Values:tensor([[ 0.0285,  0.0292,  0.0062,  ..., -0.0097, -0.0247, -0.0325],\n",
            "        [ 0.0130, -0.0048, -0.0289,  ...,  0.0343,  0.0008,  0.0102]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer:linear_relu_stack.0.bias|size:torch.Size([512])|Values:tensor([-0.0155, -0.0253], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer:linear_relu_stack.2.weight|size:torch.Size([512, 512])|Values:tensor([[ 0.0438,  0.0409, -0.0031,  ..., -0.0074,  0.0001,  0.0227],\n",
            "        [ 0.0203, -0.0249,  0.0312,  ...,  0.0276,  0.0158,  0.0203]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer:linear_relu_stack.2.bias|size:torch.Size([512])|Values:tensor([-0.0288,  0.0147], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer:linear_relu_stack.4.weight|size:torch.Size([10, 512])|Values:tensor([[-0.0433,  0.0172, -0.0076,  ...,  0.0058, -0.0138, -0.0308],\n",
            "        [-0.0392, -0.0236,  0.0356,  ...,  0.0151, -0.0390,  0.0002]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer:linear_relu_stack.4.bias|size:torch.Size([10])|Values:tensor([0.0293, 0.0085], grad_fn=<SliceBackward0>)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}